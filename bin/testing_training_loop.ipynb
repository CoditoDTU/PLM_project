{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import from loss functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#For knowledge distillation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def pad_to_match(teacher_kernel, student_kernel):\n",
    "    \"\"\"\n",
    "    Just a precaution function. It assures that tokens embeddings in both teacher and student\n",
    "    representations have the same shape. This will apply zero-padding the kernel with less dimensions.\n",
    "    \"\"\"\n",
    "    rows = max(teacher_kernel.shape[0], student_kernel.shape[0])\n",
    "    cols = max(teacher_kernel.shape[1], student_kernel.shape[1])\n",
    "    new_teacher_kernel = functional.pad(teacher_kernel, (0, cols - teacher_kernel.shape[1], \n",
    "                                                            0, rows - teacher_kernel.shape[0]))\n",
    "    new_student_kernel = functional.pad(student_kernel, (0, cols - student_kernel.shape[1], \n",
    "                                                            0, rows - student_kernel.shape[0]))\n",
    "    return new_teacher_kernel, new_student_kernel\n",
    "\n",
    "\n",
    "def kernel_similarity_matrix(kernel):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between each pair of token embeddings on the kernel\n",
    "    \"\"\"\n",
    "    return cosine_similarity(kernel.cpu().detach().numpy())\n",
    "\n",
    "def kernel_mse_alignment_loss(teacher_kernel, student_kernel):\n",
    "    \"\"\"\n",
    "    Calculates the MSE kernel alignment loss between teacher and student\n",
    "    \"\"\"\n",
    "    teacher_matrix = torch.tensor(kernel_similarity_matrix(teacher_kernel))\n",
    "    student_matrix = torch.tensor(kernel_similarity_matrix(student_kernel))\n",
    "\n",
    "    if teacher_matrix.shape != student_matrix.shape:\n",
    "        teacher_matrix, student_matrix = pad_to_match(teacher_matrix, student_matrix)\n",
    "\n",
    "    return mse_loss(teacher_matrix, student_matrix)\n",
    "\n",
    "def logits_mse_loss(teacher_logits, student_logits):\n",
    "    \"\"\"\n",
    "    Calculates the MSE loss between teacher and student logits\n",
    "    \"\"\"\n",
    "    return mse_loss(teacher_logits, student_logits)\n",
    "\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, weight_rep=1.0, weight_logits=1.0):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.weight_rep = weight_rep\n",
    "        self.weight_logits = weight_logits\n",
    "\n",
    "    def forward(self, teacher_rep, teacher_logits, student_rep, student_logits):\n",
    "\n",
    "        alignment_loss = kernel_mse_alignment_loss(teacher_rep, student_rep)\n",
    "        logits_loss = logits_mse_loss(teacher_logits, student_logits)\n",
    "        return self.weight_rep * alignment_loss + self.weight_logits * logits_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import from proteus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_rep(results, batch_lens):\n",
    "    \"\"\"\n",
    "    Get sequence representations from esm_compute\n",
    "    \"\"\"\n",
    "    print(len(results[\"representations\"]))\n",
    "    token_representations = results[\"representations\"][33]\n",
    " \n",
    "    # Generate per-sequence representations via averaging\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1: tokens_len - 1].mean(0))\n",
    " \n",
    "    return sequence_representations\n",
    " \n",
    "\n",
    "def get_logits(results):\n",
    "    \"\"\"\n",
    "    Get logits from esm_compute\n",
    "    \"\"\"\n",
    "    logits = results[\"logits\"]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import from dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import esm\n",
    "\n",
    "\n",
    "def dict_collate_fn(batch):\n",
    "# Return the batch as-is (a list of dictionaries)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def connect_db():\n",
    "    # Connect to MongoDB with connection string\n",
    "    string_path = \"mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000&appName=mongosh+2.3.3\"\n",
    "    \n",
    "    client = MongoClient(string_path)\n",
    "\n",
    "    # Create db\n",
    "    db = client['proteins']\n",
    "\n",
    "    # Access collection\n",
    "    collection = db['uniref_test']\n",
    "\n",
    "    return client, db, collection\n",
    "\n",
    "\n",
    "def get_taxon_sequence_data(collection):\n",
    "\n",
    "    # Define the query filter to exclude documents where sequence.length > 1024\n",
    "    query = {\n",
    "        \"sequence.length\": {\"$lte\": 1024}\n",
    "    }\n",
    "    projection = {\n",
    "    \"primaryAccession\": 1,    \n",
    "    \"organism.taxonId\": 1,   # Include taxonId from the organism field\n",
    "    \"sequence.value\": 1,      # Include value from the sequence field\n",
    "    \"sequence.length\": 1      # Include length from the sequence field\n",
    "    }\n",
    "    documents =  collection.find(query, projection)\n",
    "\n",
    "    minimal_documents = [] # Initialize new empty dictionary\n",
    "\n",
    "    for doc in documents:\n",
    "    # Create a new dictionary with only the desired properties\n",
    "        new_obj = {\n",
    "            \"primaryAccession\": doc.get(\"primaryAccession\",{}),\n",
    "            \"taxonId\": doc.get(\"organism\", {}).get(\"taxonId\"),\n",
    "            \"value\": doc.get(\"sequence\", {}).get(\"value\"),\n",
    "            \"length\": doc.get(\"sequence\", {}).get(\"length\")\n",
    "        }\n",
    "        minimal_documents.append(new_obj)\n",
    "\n",
    "    return minimal_documents\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        item = self.data[index]\n",
    "\n",
    "        # Extract relevant fields\n",
    "        sequence = item['value']  # The protein sequence\n",
    "        length = item['length']   # Length of the sequence\n",
    "        taxon_id = item['taxonId']  # Taxon ID\n",
    "        primary_accession = item['primaryAccession']  # Primary accession\n",
    "\n",
    "        return {\n",
    "            'sequence': sequence,  # Return sequence as a string (you could modify this later)\n",
    "            'length': length,\n",
    "            'taxon_id': taxon_id,\n",
    "            'primary_accession': primary_accession\n",
    "        }\n",
    "\n",
    "\n",
    "class TaxonIdSampler(Sampler):\n",
    "    def __init__(self, dataset: Dataset, batch_size, length_bin_size=5, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.length_bin_size = length_bin_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Group sample indices by taxonId\n",
    "        self.taxon_length_bins = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        for idx, sample in enumerate(dataset):\n",
    "            taxon_id = sample['taxon_id']\n",
    "            sequence_length = sample['length']\n",
    "            length_bin = (sequence_length // length_bin_size) * length_bin_size  # integer division to know in which bucket the sequence is\n",
    "\n",
    "            # Ensure that length_bin is properly initialized\n",
    "            self.taxon_length_bins[taxon_id][length_bin].append(idx)\n",
    "\n",
    "        '''\n",
    "        structure of self.taxon_length_bins:\n",
    "\n",
    "        {\n",
    "            taxon_id_1: {\n",
    "                length_bin_1: [sample_idx_1, sample_idx_2, ...],\n",
    "                length_bin_2: [sample_idx_3, sample_idx_4, ...],\n",
    "                ...\n",
    "            },\n",
    "            taxon_id_2: {\n",
    "                length_bin_3: [sample_idx_5, sample_idx_6, ...],\n",
    "                ...\n",
    "            },\n",
    "        }\n",
    "        '''\n",
    "        \n",
    "        # Prepare batches based on taxon groups\n",
    "        self.batches = []\n",
    "\n",
    "        for taxon, length_bins in self.taxon_length_bins.items():\n",
    "            for length_bin, indices in length_bins.items():\n",
    "                if self.shuffle:\n",
    "                    random.shuffle(indices)  # Shuffle the indices if needed\n",
    "                for i in range(0, len(indices), batch_size):\n",
    "                    self.batches.append(indices[i:i + batch_size])\n",
    "\n",
    "        # Shuffle the batches if needed\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.batches)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare batches and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kacper\\AppData\\Local\\Temp\\ipykernel_27052\\3436282648.py:38: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  collection = pd.read_json(json.dumps(json_data))\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t12_35M_UR50D.pt\" to C:\\Users\\Kacper/.cache\\torch\\hub\\checkpoints\\esm2_t12_35M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t12_35M_UR50D-contact-regression.pt\" to C:\\Users\\Kacper/.cache\\torch\\hub\\checkpoints\\esm2_t12_35M_UR50D-contact-regression.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\" to C:\\Users\\Kacper/.cache\\torch\\hub\\checkpoints\\esm2_t6_8M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t6_8M_UR50D-contact-regression.pt\" to C:\\Users\\Kacper/.cache\\torch\\hub\\checkpoints\\esm2_t6_8M_UR50D-contact-regression.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device:  cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "learning_rate = 1e-4\n",
    "weight_rep = 0.5\n",
    "weight_logits = 0.5\n",
    "\n",
    "checkpoints = True\n",
    "cp_dir = \"checkpoints\"\n",
    "cp_freq = 200\n",
    "\n",
    "# get data\n",
    "#_, _, collection = connect_db()\n",
    "#dataset = ProteinDataset(get_taxon_sequence_data(collection))\n",
    "\n",
    "############################ TEMP FOR TESTING\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_taxon_sequence_data2(collection):\n",
    "\n",
    "    documents = collection['results']\n",
    "\n",
    "    minimal_documents = [] # Initialize new empty dictionary\n",
    "\n",
    "    for doc in documents:\n",
    "    # Create a new dictionary with only the desired properties\n",
    "        new_obj = {\n",
    "            \"primaryAccession\": doc.get(\"primaryAccession\",{}),\n",
    "            \"taxonId\": doc.get(\"organism\", {}).get(\"taxonId\"),\n",
    "            \"value\": doc.get(\"sequence\", {}).get(\"value\"),\n",
    "            \"length\": doc.get(\"sequence\", {}).get(\"length\")\n",
    "        }\n",
    "        minimal_documents.append(new_obj)\n",
    "    return minimal_documents\n",
    "\n",
    "with open('../data/processed/uniref100_test.json', 'r') as file :\n",
    "    json_data = json.load(file)\n",
    "collection = pd.read_json(json.dumps(json_data))\n",
    "dataset = ProteinDataset(get_taxon_sequence_data2(collection))\n",
    "############################################\n",
    "\n",
    "sampler = TaxonIdSampler(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_sampler=sampler, collate_fn=dict_collate_fn)\n",
    "\n",
    "# load models\n",
    "teacher_model, alphabet = esm.pretrained.esm2_t12_35M_UR50D()\n",
    "student_model, _ = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "\n",
    "# initialize batch converter\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# train only student\n",
    "teacher_model.eval()\n",
    "student_model.train()\n",
    "\n",
    "# Detect device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Available device: \", device)\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "distillation_loss = DistillationLoss(weight_rep=1.0, weight_logits=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "33",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m     teacher_res \u001b[38;5;241m=\u001b[39m teacher_model(batch_tokens, repr_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m33\u001b[39m], return_contacts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m     teacher_logits \u001b[38;5;241m=\u001b[39m get_logits(teacher_res)\n\u001b[1;32m---> 33\u001b[0m     teacher_reps \u001b[38;5;241m=\u001b[39m \u001b[43mget_seq_rep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# forward pass - student\u001b[39;00m\n\u001b[0;32m     36\u001b[0m student_res \u001b[38;5;241m=\u001b[39m student_model(batch_tokens, repr_layers\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m6\u001b[39m], return_contacts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m, in \u001b[0;36mget_seq_rep\u001b[1;34m(results, batch_lens)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mGet sequence representations from esm_compute\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m----> 6\u001b[0m token_representations \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepresentations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m33\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Generate per-sequence representations via averaging\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sequence_representations \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyError\u001b[0m: 33"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        # extract sequences and names from the batch\n",
    "        sequences = [item['sequence'] for item in batch]\n",
    "        names = [item['primary_accession'] for item in batch]\n",
    "\n",
    "        # prepare data for batch conversion\n",
    "        if names is None:\n",
    "            names = [f'seq{i}' for i in range(len(sequences))]\n",
    "        data = list(zip(names, sequences))\n",
    "\n",
    "        # check datatype of sequences - str or biotite\n",
    "        if all(isinstance(x[0], str) and isinstance(x[1], str) for x in data):\n",
    "            pass  # all elements are strings\n",
    "        else:\n",
    "            data = [(x[0], str(x[1])) for x in data]\n",
    "\n",
    "        # convert data to batch tensors\n",
    "        batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "        batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "        batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass - teacher\n",
    "        with torch.no_grad():\n",
    "            teacher_res = teacher_model(batch_tokens, repr_layers=[33], return_contacts=False)\n",
    "            teacher_logits = get_logits(teacher_res)\n",
    "            teacher_reps = get_seq_rep(teacher_res, batch_lens)\n",
    "\n",
    "        # forward pass - student\n",
    "        student_res = student_model(batch_tokens, repr_layers=[6], return_contacts=False)\n",
    "        student_logits = get_logits(student_res)\n",
    "        student_reps = get_seq_rep(student_res, batch_lens)\n",
    "\n",
    "        # compute loss and backprop\n",
    "        loss = distillation_loss(teacher_reps, teacher_logits, student_reps, student_logits)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "    # tensorflow-like checkpoints\n",
    "    if checkpoints:\n",
    "        if (epoch + 1) % cp_freq == 0:\n",
    "            path = f'cp_epoch_{epoch+1}.pt'\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': student_model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "            }, path)\n",
    "            print(f'Checkpoint saved: {path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_plm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
